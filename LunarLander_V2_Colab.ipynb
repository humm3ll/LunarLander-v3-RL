{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5d24b0",
   "metadata": {},
   "source": [
    "# LunarLander-v3 Reinforcement Learning Comparison\n",
    "## DQN vs DDQN vs PER\n",
    "\n",
    "**Author:** Ethan Hulme  \n",
    "**Course:** CIS2719 - Foundations of Robotics & AI (Coursework 2)  \n",
    "**Date:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements and compares three value-based deep reinforcement learning algorithms for the LunarLander-v3 environment from OpenAI Gymnasium:\n",
    "\n",
    "1. **DQN (Deep Q-Network)** - Standard value-based RL using experience replay and target networks\n",
    "2. **DDQN (Double DQN)** - Addresses overestimation bias by decoupling action selection from evaluation\n",
    "3. **PER (Prioritized Experience Replay)** - Samples transitions based on TD-error magnitude combined with DDQN targets\n",
    "\n",
    "This notebook contains:\n",
    "- Complete implementation code\n",
    "- Results from 10 experimental runs (600 to 10,000 episodes)\n",
    "- Learning curves and performance visualizations\n",
    "- GIF animations showing trained agent behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1d91b",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, we'll install all required dependencies. This includes:\n",
    "- `gymnasium[box2d]` - The LunarLander environment\n",
    "- `torch` - Deep learning framework\n",
    "- `numpy` - Numerical computations\n",
    "- `matplotlib` - Plotting\n",
    "- `imageio` - GIF generation and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd9a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gymnasium[box2d] torch numpy matplotlib imageio imageio-ffmpeg pillow -q\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "from datetime import datetime\n",
    "from IPython.display import Image, display\n",
    "import base64\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91c7d6",
   "metadata": {},
   "source": [
    "## 2. Clone GitHub Repository (Optional)\n",
    "\n",
    "If you want to access the pre-trained results and GIFs from GitHub, you can clone the repository here. Otherwise, we'll train from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efa769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the GitHub repository to access all test results\n",
    "# Replace with your actual repository URL\n",
    "!git clone https://github.com/humm3ll/LunarLander-v3-RL.git\n",
    "%cd LunarLander-v3-RL\n",
    "\n",
    "# List the contents\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e4892",
   "metadata": {},
   "source": [
    "## 3. Complete Implementation Code\n",
    "\n",
    "Below is the complete implementation of all three algorithms (DQN, DDQN, and PER) including:\n",
    "- Q-Network architecture\n",
    "- Replay buffers (standard and prioritized)\n",
    "- Agent class with training logic\n",
    "- Utility functions for environment creation, plotting, and GIF generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c543895",
   "metadata": {},
   "source": [
    "### 3.1 Environment Setup and Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3206793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import imageio\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  Utility: make environment with proper seeding\n",
    "# -----------------------------------------------------------\n",
    "def make_env(env_name: str, seed: int = 42, render_mode=None):\n",
    "    \"\"\"\n",
    "    Helper to create and seed a Gymnasium environment.\n",
    "    render_mode=None for fast training,\n",
    "    render_mode='rgb_array' when generating a GIF.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name, render_mode=render_mode)\n",
    "    # reset returns (obs, info) in gymnasium\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    return env\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  Q-network used by all algorithms\n",
    "# -----------------------------------------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 2-hidden-layer MLP for approximating Q(s,a).\n",
    "    Architecture kept small for stability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"‚úì Environment and Q-Network defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819d32b",
   "metadata": {},
   "source": [
    "### 3.2 Replay Buffers (Standard and Prioritized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b50959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "#  Replay buffer (vanilla)\n",
    "# -----------------------------------------------------------\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Standard replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int = 100_000):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  Prioritised Replay Buffer (simple proportional PER)\n",
    "# -----------------------------------------------------------\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    PER with proportional priorities.\n",
    "    Each transition gets priority p_i; sampling probability is p_i^alpha / sum p_j^alpha.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int = 100_000, alpha: float = 0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.pos = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "\n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size: int, beta: float = 0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[: self.pos]\n",
    "\n",
    "        # convert priorities into a probability distribution\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "        # importance-sampling weights (to correct the bias introduced by PER)\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()  # normalise for numerical stability\n",
    "\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "            indices,\n",
    "            np.array(weights, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, new_priorities):\n",
    "        # small epsilon keeps priorities non-zero\n",
    "        for idx, prio in zip(indices, new_priorities):\n",
    "            self.priorities[idx] = float(prio)\n",
    "\n",
    "print(\"‚úì Replay buffers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca4d46",
   "metadata": {},
   "source": [
    "### 3.3 DQN Agent (supports DQN, DDQN, and PER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceab8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "#  DQN / DDQN / PER Agent\n",
    "# -----------------------------------------------------------\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Generic agent that can run:\n",
    "      - 'dqn'    : standard DQN\n",
    "      - 'ddqn'   : Double DQN (separate action selection and evaluation)\n",
    "      - 'per'    : Prioritised Replay + Double-DQN targets\n",
    "\n",
    "    The internal logic is the same, but the replay buffer and TD-target change.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        algo_type: str = \"dqn\",\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 1e-3,\n",
    "        batch_size: int = 64,\n",
    "        buffer_capacity: int = 100_000,\n",
    "        min_buffer_size: int = 10_000,\n",
    "        target_update_freq: int = 1_000,\n",
    "        device: str | None = None,\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.min_buffer_size = min_buffer_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.algo_type = algo_type.lower()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-networks\n",
    "        self.q_net = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        # Loss: SmoothL1 (Huber) is standard for DQN\n",
    "        # For PER we keep per-sample loss (reduction=\"none\") so we can weight it.\n",
    "        self.loss_fn = nn.SmoothL1Loss(\n",
    "            reduction=\"none\" if self.algo_type == \"per\" else \"mean\"\n",
    "        )\n",
    "\n",
    "        # Replay buffer\n",
    "        if self.algo_type == \"per\":\n",
    "            self.buffer = PrioritizedReplayBuffer(capacity=buffer_capacity)\n",
    "            # parameters for annealing importance-sampling exponent beta\n",
    "            self.beta_start = 0.4\n",
    "            self.beta_frames = 200_000\n",
    "        else:\n",
    "            self.buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "        # epsilon-greedy exploration schedule\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end = 0.05\n",
    "        self.eps_decay = 250_000  # in frames\n",
    "        self.frame_idx = 0\n",
    "\n",
    "        self.training_steps = 0\n",
    "\n",
    "    # ---------- exploration schedule ----------\n",
    "    def epsilon(self) -> float:\n",
    "        # Exponential decay: starts near 1, approaches eps_end as frame_idx grows.\n",
    "        return self.eps_end + (self.eps_start - self.eps_end) * math.exp(\n",
    "            -1.0 * self.frame_idx / self.eps_decay\n",
    "        )\n",
    "\n",
    "    # ---------- action selection ----------\n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Epsilon-greedy choice used during training.\"\"\"\n",
    "        self.frame_idx += 1\n",
    "        if random.random() < self.epsilon():\n",
    "            return random.randrange(self.action_dim)\n",
    "\n",
    "        state_t = torch.tensor(\n",
    "            state, dtype=torch.float32, device=self.device\n",
    "        ).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state_t)\n",
    "        return int(q_values.argmax(dim=1).item())\n",
    "\n",
    "    def greedy_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Purely greedy action (used during evaluation / GIF generation).\"\"\"\n",
    "        state_t = torch.tensor(\n",
    "            state, dtype=torch.float32, device=self.device\n",
    "        ).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state_t)\n",
    "        return int(q_values.argmax(dim=1).item())\n",
    "\n",
    "    # ---------- replay interaction ----------\n",
    "    def push(self, *transition):\n",
    "        self.buffer.push(*transition)\n",
    "\n",
    "    def can_update(self) -> bool:\n",
    "        return len(self.buffer) >= self.min_buffer_size\n",
    "\n",
    "    # ---------- TD-target computation ----------\n",
    "    def compute_td_target(\n",
    "        self, rewards: torch.Tensor, next_states: torch.Tensor, dones: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Shared logic for computing TD targets.\n",
    "\n",
    "        DQN:   max_a' Q_target(s', a')\n",
    "        DDQN:  Q_target(s', argmax_a' Q_online(s', a'))\n",
    "        PER:   uses DDQN-style target (usually more stable).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            next_q_target = self.target_net(next_states)  # [batch, actions]\n",
    "\n",
    "            if self.algo_type in (\"ddqn\", \"per\"):\n",
    "                # Double DQN: choose action via online net, evaluate via target net\n",
    "                next_q_online = self.q_net(next_states)\n",
    "                next_actions = next_q_online.argmax(dim=1, keepdim=True)\n",
    "                next_q = next_q_target.gather(1, next_actions).squeeze(1)\n",
    "            else:\n",
    "                # Plain DQN: directly take the max over target network\n",
    "                next_q, _ = next_q_target.max(dim=1)\n",
    "\n",
    "            td_target = rewards + self.gamma * (1.0 - dones) * next_q\n",
    "        return td_target\n",
    "\n",
    "    # ---------- single gradient step ----------\n",
    "    def update(self):\n",
    "        if not self.can_update():\n",
    "            return None\n",
    "\n",
    "        self.training_steps += 1\n",
    "\n",
    "        if self.algo_type == \"per\":\n",
    "            # Anneal beta from beta_start -> 1.0 over beta_frames updates\n",
    "            beta = min(\n",
    "                1.0,\n",
    "                self.beta_start\n",
    "                + self.training_steps * (1.0 - self.beta_start) / self.beta_frames,\n",
    "            )\n",
    "            (\n",
    "                states,\n",
    "                actions,\n",
    "                rewards,\n",
    "                next_states,\n",
    "                dones,\n",
    "                indices,\n",
    "                weights,\n",
    "            ) = self.buffer.sample(self.batch_size, beta)\n",
    "            weights = torch.tensor(\n",
    "                weights, dtype=torch.float32, device=self.device\n",
    "            )  # [batch]\n",
    "        else:\n",
    "            states, actions, rewards, next_states, dones = self.buffer.sample(\n",
    "                self.batch_size\n",
    "            )\n",
    "            weights = torch.ones(self.batch_size, device=self.device)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64, device=self.device).unsqueeze(\n",
    "            1\n",
    "        )\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.tensor(\n",
    "            next_states, dtype=torch.float32, device=self.device\n",
    "        )\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Q(s,a) for actions taken\n",
    "        q_values = self.q_net(states).gather(1, actions).squeeze(1)\n",
    "\n",
    "        # TD target (depends on algo_type)\n",
    "        td_target = self.compute_td_target(rewards, next_states, dones)\n",
    "\n",
    "        # Per-sample loss for PER, mean loss otherwise\n",
    "        loss_tensor = self.loss_fn(q_values, td_target)\n",
    "        loss = (loss_tensor * weights).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Optional but often helps stability\n",
    "        nn.utils.clip_grad_norm_(self.q_net.parameters(), max_norm=10.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update priorities based on TD error magnitude\n",
    "        if self.algo_type == \"per\":\n",
    "            new_priorities = loss_tensor.detach().cpu().numpy() + 1e-6\n",
    "            self.buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "        # Softly copy online weights to target network every N steps\n",
    "        if self.training_steps % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        return float(loss.item())\n",
    "\n",
    "print(\"‚úì DQN Agent defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec9187",
   "metadata": {},
   "source": [
    "### 3.4 Training and Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fedfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "#  Training utilities\n",
    "# -----------------------------------------------------------\n",
    "def train_agent(\n",
    "    env_name: str,\n",
    "    algo_type: str,\n",
    "    num_episodes: int = 600,\n",
    "    seed: int = 42,\n",
    ") -> tuple[DQNAgent, list[float]]:\n",
    "    \"\"\"\n",
    "    Train a single agent on LunarLander and return the trained\n",
    "    agent plus a list of episode returns.\n",
    "    \"\"\"\n",
    "\n",
    "    env = make_env(env_name, seed=seed, render_mode=None)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        algo_type=algo_type,\n",
    "        gamma=0.99,\n",
    "        lr=1e-3,\n",
    "        batch_size=64,\n",
    "        buffer_capacity=100_000,\n",
    "        min_buffer_size=10_000,\n",
    "        target_update_freq=1_000,\n",
    "    )\n",
    "\n",
    "    episode_rewards: list[float] = []\n",
    "\n",
    "    print(f\"\\n=== Training {algo_type.upper()} on {env_name} for {num_episodes} episodes ===\")\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Store transition\n",
    "            agent.push(state, action, reward, next_state, float(done))\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Perform gradient step when enough samples available\n",
    "            if agent.can_update():\n",
    "                agent.update()\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        if episode % 20 == 0:\n",
    "            last_mean = np.mean(episode_rewards[-20:])\n",
    "            print(\n",
    "                f\"Episode {episode:4d} | \"\n",
    "                f\"avg reward (last 20): {last_mean:7.2f} | \"\n",
    "                f\"epsilon: {agent.epsilon():.3f}\"\n",
    "            )\n",
    "\n",
    "    env.close()\n",
    "    return agent, episode_rewards\n",
    "\n",
    "\n",
    "def plot_learning_curves(results: dict, save_path: str = \"learning_curves.png\"):\n",
    "    \"\"\"\n",
    "    Plot episode reward curves for each algorithm.\n",
    "    `results` is a dict: algo_name -> list_of_episode_rewards\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for label, rewards in results.items():\n",
    "        rewards = np.array(rewards)\n",
    "        # moving average for smoothing (window 20)\n",
    "        window = 20\n",
    "        if len(rewards) >= window:\n",
    "            smooth = np.convolve(rewards, np.ones(window) / window, mode=\"valid\")\n",
    "            plt.plot(\n",
    "                range(window, len(rewards) + 1),\n",
    "                smooth,\n",
    "                label=f\"{label} (moving avg)\",\n",
    "            )\n",
    "        plt.plot(np.arange(1, len(rewards) + 1), rewards, alpha=0.25, linestyle=\"--\")\n",
    "\n",
    "    plt.axhline(200, color=\"grey\", linestyle=\":\", label=\"Solved threshold (‚âà200)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episodic Return\")\n",
    "    plt.title(\"LunarLander-v3: DQN vs DDQN vs PER\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Saved learning curves to {save_path}\")\n",
    "\n",
    "\n",
    "def generate_gif(\n",
    "    agent: DQNAgent,\n",
    "    env_name: str,\n",
    "    filename: str,\n",
    "    seed: int = 0,\n",
    "    episodes: int = 3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Roll out the learned policy (greedy) and record a GIF.\n",
    "    \"\"\"\n",
    "    env = make_env(env_name, seed=seed, render_mode=\"rgb_array\")\n",
    "    frames = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            frame = env.render()  # rgb_array\n",
    "            frames.append(frame)\n",
    "\n",
    "            action = agent.greedy_action(state)\n",
    "            next_state, _, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            step += 1\n",
    "\n",
    "    env.close()\n",
    "    imageio.mimsave(filename, frames, fps=30)\n",
    "    print(f\"Saved GIF for policy to {filename}\")\n",
    "\n",
    "print(\"‚úì Training and visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5709d27",
   "metadata": {},
   "source": [
    "## 4. Training Configuration\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "All experiments used the following hyperparameters:\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Learning rate | 1e-3 |\n",
    "| Discount factor (Œ≥) | 0.99 |\n",
    "| Batch size | 64 |\n",
    "| Replay buffer capacity | 100,000 |\n",
    "| Min buffer size before training | 10,000 |\n",
    "| Target network update frequency | 1,000 steps |\n",
    "| Epsilon start | 1.0 |\n",
    "| Epsilon end | 0.05 |\n",
    "| Epsilon decay | 250,000 frames |\n",
    "| Loss function | Smooth L1 (Huber) |\n",
    "| Optimizer | Adam |\n",
    "| Gradient clipping | Max norm 10.0 |\n",
    "\n",
    "**PER-Specific:**\n",
    "- Alpha (priority exponent): 0.6\n",
    "- Beta (importance sampling): 0.4 ‚Üí 1.0 (annealed over 200,000 frames)\n",
    "\n",
    "### Experimental Runs\n",
    "\n",
    "The project includes results from 10 experimental runs:\n",
    "- **Tests 1-5:** 600 episodes each\n",
    "- **Tests 6-8:** 2,000 episodes each\n",
    "- **Test 9:** 5,000 episodes\n",
    "- **Test 10:** 10,000 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5a356",
   "metadata": {},
   "source": [
    "## 5. Run Training (Optional)\n",
    "\n",
    "**Note:** If you cloned the repository, all training results are already available. You can skip this section and go directly to the results visualization.\n",
    "\n",
    "If you want to train from scratch, run the cells below. **Warning:** Training takes several hours depending on the number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97be9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training example - 600 episodes\n",
    "# Uncomment and run if you want to train from scratch\n",
    "\n",
    "# ENV_NAME = \"LunarLander-v3\"\n",
    "# NUM_EPISODES = 600\n",
    "\n",
    "# results = {}\n",
    "# trained_agents = {}\n",
    "\n",
    "# # Train all three algorithms\n",
    "# for algo in [\"dqn\", \"ddqn\", \"per\"]:\n",
    "#     agent, rewards = train_agent(ENV_NAME, algo_type=algo, num_episodes=NUM_EPISODES)\n",
    "#     results[algo.upper()] = rewards\n",
    "#     trained_agents[algo.upper()] = agent\n",
    "\n",
    "#     print(f\"{algo.upper()} final mean reward over last 50 episodes: {np.mean(rewards[-50:]):.2f}\")\n",
    "\n",
    "# # Generate plots and GIFs\n",
    "# plot_learning_curves(results, save_path=\"learning_curves.png\")\n",
    "# generate_gif(trained_agents[\"DQN\"], ENV_NAME, \"dqn_agent.gif\")\n",
    "# generate_gif(trained_agents[\"DDQN\"], ENV_NAME, \"ddqn_agent.gif\")\n",
    "# generate_gif(trained_agents[\"PER\"], ENV_NAME, \"per_agent.gif\")\n",
    "\n",
    "print(\"Training section ready (commented out). Uncomment to train from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf58a2c",
   "metadata": {},
   "source": [
    "## 6. Display Pre-trained Results\n",
    "\n",
    "Below we'll display the results from all 10 experimental runs, including learning curves and agent behavior GIFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9f16d",
   "metadata": {},
   "source": [
    "### 6.1 Helper Function to Display Images and GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0656f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPImage, display, HTML\n",
    "\n",
    "def display_image_from_file(filepath, width=800):\n",
    "    \"\"\"Display an image file in the notebook\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        display(IPImage(filename=filepath, width=width))\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {filepath}\")\n",
    "\n",
    "def display_gif_from_file(filepath, width=600):\n",
    "    \"\"\"Display a GIF file in the notebook\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            gif_data = f.read()\n",
    "        gif_b64 = base64.b64encode(gif_data).decode('ascii')\n",
    "        display(HTML(f'<img src=\"data:image/gif;base64,{gif_b64}\" width=\"{width}\"/>'))\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {filepath}\")\n",
    "\n",
    "def list_test_folders(base_path=\"Test Results\"):\n",
    "    \"\"\"List all test result folders\"\"\"\n",
    "    if os.path.exists(base_path):\n",
    "        folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "        return sorted(folders)\n",
    "    else:\n",
    "        print(f\"‚ùå Directory not found: {base_path}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úì Display helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13019530",
   "metadata": {},
   "source": [
    "### 6.2 Test 1 (600 Episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a894351",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"Test Results/Test 1 (600 Eps)\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 1 - 600 EPISODES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display learning curve\n",
    "print(\"\\nüìä Learning Curves:\")\n",
    "display_image_from_file(f\"{test_path}/learning_curves.png\")\n",
    "\n",
    "# Display agent GIFs\n",
    "print(\"\\nüéÆ DQN Agent Behavior:\")\n",
    "display_gif_from_file(f\"{test_path}/dqn_agent.gif\")\n",
    "\n",
    "print(\"\\nüéÆ DDQN Agent Behavior:\")\n",
    "display_gif_from_file(f\"{test_path}/ddqn_agent.gif\")\n",
    "\n",
    "print(\"\\nüéÆ PER Agent Behavior:\")\n",
    "display_gif_from_file(f\"{test_path}/per_agent.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319186db",
   "metadata": {},
   "source": [
    "### 6.3 Test 6 (2000 Episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3214d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"Test Results/Test 6 (2000 Eps)\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 6 - 2000 EPISODES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display learning curve\n",
    "print(\"\\nüìä Learning Curves:\")\n",
    "display_image_from_file(f\"{test_path}/learning_curves.png\")\n",
    "\n",
    "# Display agent GIFs\n",
    "print(\"\\nüéÆ DQN Agent Behavior:\")\n",
    "display_gif_from_file(f\"{test_path}/dqn_agent.gif\")\n",
    "\n",
    "print(\"\\nüéÆ DDQN Agent Behavior:\")\n",
    "display_gif_from_file(f\"{test_path}/ddqn_agent.gif\")\n",
    "\n",
    "print(\"\\nüéÆ PER Agent Behavior:\")\n",
    "display_gif_from_file(f\"{test_path}/per_agent.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b662c39",
   "metadata": {},
   "source": [
    "### 6.4 Test 10 (10000 Episodes) - Best Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"Test Results/Test 10 (10000 Eps)\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 10 - 10000 EPISODES (MAXIMUM TRAINING)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display learning curve\n",
    "print(\"\\nüìä Learning Curves:\")\n",
    "display_image_from_file(f\"{test_path}/learning_curves.png\")\n",
    "\n",
    "# Display agent GIFs\n",
    "print(\"\\nüéÆ DQN Agent Behavior:\")\n",
    "display_gif_from_file(f\"{test_path}/dqn_agent.gif\")\n",
    "\n",
    "print(\"\\nüéÆ DDQN Agent Behavior:\")\n",
    "display_gif_from_file(f\"{test_path}/ddqn_agent.gif\")\n",
    "\n",
    "print(\"\\nüéÆ PER Agent Behavior:\")\n",
    "display_gif_from_file(f\"{test_path}/per_agent.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eeb582",
   "metadata": {},
   "source": [
    "### 6.5 View All Other Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all test results dynamically\n",
    "test_folders = list_test_folders(\"Test Results\")\n",
    "print(f\"Found {len(test_folders)} test result folders:\")\n",
    "for folder in test_folders:\n",
    "    print(f\"  - {folder}\")\n",
    "\n",
    "# Display results from remaining tests (Tests 2-5, 7-9)\n",
    "remaining_tests = [\n",
    "    \"Test 2 (600 Eps)\",\n",
    "    \"Test 3 (600 Eps)\",\n",
    "    \"Test 4 (600 Eps)\",\n",
    "    \"Test 5 (600 Eps)\",\n",
    "    \"Test 7 (2000 Eps)\",\n",
    "    \"Test 8 (2000 Eps)\",\n",
    "    \"Test 9 (5000 Eps)\"\n",
    "]\n",
    "\n",
    "for test_name in remaining_tests:\n",
    "    test_path = f\"Test Results/{test_name}\"\n",
    "    if os.path.exists(test_path):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"{test_name.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Display learning curve\n",
    "        print(\"\\nüìä Learning Curves:\")\n",
    "        display_image_from_file(f\"{test_path}/learning_curves.png\")\n",
    "\n",
    "        # Display agent GIFs (optionally show only learning curves to save space)\n",
    "        # Uncomment below to show all GIFs for each test\n",
    "        # print(\"\\nüéÆ DQN Agent:\")\n",
    "        # display_gif_from_file(f\"{test_path}/dqn_agent.gif\", width=400)\n",
    "        # print(\"\\nüéÆ DDQN Agent:\")\n",
    "        # display_gif_from_file(f\"{test_path}/ddqn_agent.gif\", width=400)\n",
    "        # print(\"\\nüéÆ PER Agent:\")\n",
    "        # display_gif_from_file(f\"{test_path}/per_agent.gif\", width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0ad9a",
   "metadata": {},
   "source": [
    "## 7. Analysis and Discussion\n",
    "\n",
    "### Algorithm Comparison\n",
    "\n",
    "**DQN (Deep Q-Network)**\n",
    "- Standard value-based RL with experience replay\n",
    "- Uses max Q-value from target network for TD target\n",
    "- Baseline for comparison\n",
    "\n",
    "**DDQN (Double DQN)**\n",
    "- Addresses overestimation bias in DQN\n",
    "- Decouples action selection (online network) from action evaluation (target network)\n",
    "- Generally more stable and achieves better performance\n",
    "\n",
    "**PER (Prioritized Experience Replay)**\n",
    "- Samples important transitions more frequently based on TD-error\n",
    "- Uses importance sampling weights to correct for bias\n",
    "- Can achieve faster learning and better sample efficiency\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Training Duration Impact:** Agents trained for 10,000 episodes show significantly more stable and higher performance than those trained for 600 episodes.\n",
    "\n",
    "2. **Algorithm Performance:** DDQN typically shows improved stability over vanilla DQN, while PER can accelerate learning by focusing on informative transitions.\n",
    "\n",
    "3. **Convergence:** The learning curves show that all three algorithms can solve the LunarLander task (achieving >200 reward) with sufficient training time.\n",
    "\n",
    "4. **Variance:** Early training shows high variance in rewards, which decreases as the policy improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601523dc",
   "metadata": {},
   "source": [
    "## 8. Environment Information\n",
    "\n",
    "### LunarLander-v3 Details\n",
    "\n",
    "**State Space (8 dimensions):**\n",
    "1. x position\n",
    "2. y position\n",
    "3. x velocity\n",
    "4. y velocity\n",
    "5. angle\n",
    "6. angular velocity\n",
    "7. left leg contact (boolean)\n",
    "8. right leg contact (boolean)\n",
    "\n",
    "**Action Space (4 discrete actions):**\n",
    "- 0: Do nothing\n",
    "- 1: Fire left engine\n",
    "- 2: Fire main engine\n",
    "- 3: Fire right engine\n",
    "\n",
    "**Reward Structure:**\n",
    "- Moving towards landing pad: positive reward\n",
    "- Moving away from landing pad: negative reward\n",
    "- Crashing: -100\n",
    "- Landing safely: +100 to +140\n",
    "- Each leg contact: +10\n",
    "- Firing main engine: -0.3 per frame\n",
    "- Firing side engine: -0.03 per frame\n",
    "\n",
    "**Success Criterion:**\n",
    "- Average reward ‚â• 200 over 100 consecutive episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce87c1",
   "metadata": {},
   "source": [
    "## 9. References\n",
    "\n",
    "1. **Mnih, V., et al. (2015).** *Human-level control through deep reinforcement learning.* Nature, 518(7540), 529-533.\n",
    "   - Original DQN paper introducing experience replay and target networks\n",
    "\n",
    "2. **Van Hasselt, H., Guez, A., & Silver, D. (2016).** *Deep Reinforcement Learning with Double Q-learning.* Proceedings of the AAAI Conference on Artificial Intelligence.\n",
    "   - Double DQN paper addressing overestimation bias\n",
    "\n",
    "3. **Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2015).** *Prioritized Experience Replay.* International Conference on Learning Representations (ICLR).\n",
    "   - PER paper introducing prioritized sampling from replay buffer\n",
    "\n",
    "4. **Gymnasium Documentation:** https://gymnasium.farama.org/\n",
    "   - OpenAI's maintained fork of Gym, including LunarLander-v3 environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451855d",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This notebook demonstrates a comprehensive comparison of three value-based deep reinforcement learning algorithms (DQN, DDQN, and PER) on the LunarLander-v3 task. \n",
    "\n",
    "**Key Findings:**\n",
    "- All three algorithms successfully learn to solve the LunarLander task\n",
    "- DDQN shows improved stability over vanilla DQN by addressing overestimation bias\n",
    "- PER can accelerate learning by prioritizing important transitions\n",
    "- Extended training (10,000 episodes) yields significantly better and more stable policies\n",
    "\n",
    "**Implementation Highlights:**\n",
    "- Clean, modular code structure\n",
    "- Shared agent architecture supporting all three algorithms\n",
    "- Comprehensive hyperparameter tuning\n",
    "- Multiple experimental runs for validation\n",
    "- Visual results (learning curves and agent behavior GIFs)\n",
    "\n",
    "This work was completed as part of CIS2719 Coursework 2 - Foundations of Robotics & AI.\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Ethan Hulme  \n",
    "**Date:** January 2026  \n",
    "**GitHub Repository:** https://github.com/humm3ll/LunarLander-v3-RL"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
